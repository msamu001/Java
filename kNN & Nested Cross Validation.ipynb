{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load the IRIS dataset, as in the labs\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "#Set X equal to features, Y equal to the targets\n",
    "X=iris.data \n",
    "y=iris.target\n",
    "\n",
    "mySeed=1234567\n",
    "np.random.seed(mySeed) # initialize random seed to replicate results over different runs\n",
    "XN=X+np.random.normal(0,0.5,X.shape) # add noise to data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Implementing kNN\n",
    "\n",
    "In the cell below, develop your own code for performing k-Nearest Neighbour classification.  You are guided to do this by following the notebook in Lab 4.  Define a function that performs k-NN given a set of data.  Your function should be invoked similary to:\n",
    "\n",
    "        y_ = mykNN(X,y,X_,options)\n",
    "        \n",
    "where X is your training data, y is your training outputs, X\\_ are your testing data and y\\_ are your predicted outputs for X\\_.  The options argument (can be a list or a set of separate arguments depending on how you choose to implement the function) should at least contain the number of neighbours to consider as well as the distance function employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X : Training data\n",
    "# y : Training targets\n",
    "# X_ : Testing data\n",
    "# option : [n neighbours, distance metric]\n",
    "\n",
    "def mykNN(X,y,X_,options):\n",
    "    y_pred = []\n",
    "    \n",
    "    ## Distance Metrics calculate the distance between both arguments\n",
    "    def euclidean(in1,in2):\n",
    "        ans = 0.0\n",
    "        for i in range(0,len(in1)):\n",
    "            ans += (in1[i] - in2[i])**2\n",
    "        return ans**0.5\n",
    "\n",
    "    def manhatten(in1,in2):\n",
    "        ans = 0.0\n",
    "        for i in range(0,len(in1)):\n",
    "            ans += abs(in1[i] - in2[i])\n",
    "        return ans\n",
    "    \n",
    "    def chebyshev(in1, in2):\n",
    "        ans = 0.0\n",
    "        for i in range(0,len(in1)):\n",
    "            a = abs(in1[i] - in2[i])\n",
    "            if(a > ans):\n",
    "                ans = a\n",
    "        return ans\n",
    "    \n",
    "    def minkowski(in1, in2, p):\n",
    "        ans = 0.0\n",
    "        for i in range(0,len(in1)):\n",
    "            ans += (abs(in1[i] - in2[i])**p)\n",
    "        return ans**(1/p)\n",
    "    ## Distance Metrics\n",
    "    \n",
    "    \n",
    "    def getNeighbours(X,X_,options):\n",
    "        dists = []\n",
    "        nIndicies = [0]\n",
    "        \n",
    "        # Calculate distances using the metric\n",
    "        if(options[1] == 0):\n",
    "            for i in range(0,len(X)):\n",
    "                dists.append(euclidean(X_,X[i]))\n",
    "        elif(options[1] == 1):\n",
    "            for i in range(0,len(X)):\n",
    "                dists.append(manhatten(X_,X[i]))\n",
    "        elif(options[1] == 2):\n",
    "            for i in range(0,len(X)):\n",
    "                dists.append(chebyshev(X_,X[i]))        \n",
    "        else:\n",
    "            for i in range(0,len(X)):\n",
    "                dists.append(minkowski(X_,X[i]))\n",
    "    \n",
    "        # Builds list from smallest distance (list[0]) to largest (list[-1])\n",
    "        for i in range(1,len(X)):\n",
    "            if(dists[i] < dists[nIndicies[-1]]):\n",
    "                index = -2\n",
    "                extend = 'True'\n",
    "                for j in range(1,len(nIndicies)):\n",
    "                    if(dists[i] > dists[nIndicies[index]]):\n",
    "                        nIndicies.insert(index+1, i)\n",
    "                        extend = 'False'\n",
    "                        break\n",
    "                    index -= 1                    \n",
    "                if(extend=='True'):\n",
    "                    nIndicies.insert(0, i)\n",
    "            \n",
    "                if(len(nIndicies) > options[0]):\n",
    "                    del nIndicies[-1]\n",
    "            else:\n",
    "                if(len(nIndicies) < options[0]):\n",
    "                    nIndicies.append(i)\n",
    "        return nIndicies # indices of n-nearest neighbours in training data\n",
    "\n",
    "\n",
    "    def assignLabel(nLabels):\n",
    "        if(len(nLabels) == 1):\n",
    "            return nLabels[0]\n",
    "        labelNum = []\n",
    "        label = 0\n",
    "        largest = 0\n",
    "        for i in range(0,len(np.unique(y))):\n",
    "            labelNum.append(0)\n",
    "        for i in range(0, len(nLabels)):\n",
    "            labelNum[nLabels[i]] += 1\n",
    "        for i in range(0,len(labelNum)):\n",
    "            if(labelNum[i] > largest):\n",
    "                largest = labelNum[i]\n",
    "                label = i\n",
    "        return label # label assigned to test point x_\n",
    "    \n",
    "    for i in range(0,len(X_)):\n",
    "        n = getNeighbours(X,X_[i],options)\n",
    "        y_pred.append(assignLabel(y[n]))\n",
    "        \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nested Cross-validation using your implementation of KNN\n",
    "\n",
    "In the cell below, develop your own code for performing 5-fold nested cross-validation along with your implemenation of k-NN above.  Again, you are guided to complete this task by following the appropriate notebook in Lab 4.  Your code for nested cross-validation should invoke your kNN function (see above).  You cross validation function should be invoked similary to:\n",
    "\n",
    "    accuracy_fold=myNestedCrossVal(X,y,5,list(range(1,11)),['euclidean','manhattan'],mySeed)\n",
    "    \n",
    "where X is your data matrix (containing all samples and features for each sample), 5 is the number of folds, y are your known output labels, ``list(range(1,11)`` evaluates the neighbour parameter from 1 to 10, and ``['euclidean','manhattan']`` evaluates the two distances on the validation sets.  mySeed is simply a random seed to enable us to replicate your results.\n",
    "\n",
    "**Notes:** \n",
    "- you should perform nested cross-validation on both your original data X, as well as the data pertrubed by noise as shown in the cells above (XN)\n",
    "- you should implement/validate at least two distance functions\n",
    "- you should evaluate number of neighbours from 1 to 10\n",
    "- your function should return a list of accuracies per fold\n",
    "- for each fold, your function should print:\n",
    "  - the accuracy per distinct set of parameters on the validation set\n",
    "  - the best set of parameters for the fold after validation\n",
    "  - the confusion matrix per fold (on the testing set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myAccuracy(y_test,y_pred):\n",
    "    accuracy = (len(y_test) - len(np.where(y_test != y_pred)[0])) / len(y_test)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def myConfMat(y_test,y_pred,classNum):\n",
    "    C = np.zeros((classNum,classNum))  # initialize the confusion matrix to zeros\n",
    "    \n",
    "    #loops through all results and update the confusion matrix\n",
    "    for i in range(0, len(y_test)):\n",
    "        C[y_test[i]][y_pred[i]] += 1\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Original Data === \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  3 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  0.9666666666666667 \n",
      "\n",
      " Confusion Martix: \n",
      " [[11.  0.  0.]\n",
      " [ 0. 10.  1.]\n",
      " [ 0.  0.  8.]] \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  4 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  0.9333333333333333 \n",
      "\n",
      " Confusion Martix: \n",
      " [[12.  0.  0.]\n",
      " [ 0. 11.  0.]\n",
      " [ 0.  2.  5.]] \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  1 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  0.9333333333333333 \n",
      "\n",
      " Confusion Martix: \n",
      " [[ 5.  0.  0.]\n",
      " [ 0. 13.  1.]\n",
      " [ 0.  1. 10.]] \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  4 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  1.0 \n",
      "\n",
      " Confusion Martix: \n",
      " [[11.  0.  0.]\n",
      " [ 0.  7.  0.]\n",
      " [ 0.  0. 12.]] \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  1 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  0.9666666666666667 \n",
      "\n",
      " Confusion Martix: \n",
      " [[11.  0.  0.]\n",
      " [ 0.  6.  1.]\n",
      " [ 0.  0. 12.]] \n",
      "\n",
      " Total Confusion Matrix:  \n",
      " [[50.  0.  0.]\n",
      " [ 0. 47.  3.]\n",
      " [ 0.  3. 47.]] \n",
      "\n",
      " Accuracy List:  \n",
      " [0.9666666666666667, 0.9333333333333333, 0.9333333333333333, 1.0, 0.9666666666666667] \n",
      "\n",
      " Mean:  0.96 \n",
      "\n",
      " Stadard Deviation:  0.024944382578492935\n",
      "____________________________________________________________________________________________________\n",
      "\n",
      "\n",
      "=== Noisy Data === \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  1 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  0.7333333333333333 \n",
      "\n",
      " Confusion Martix: \n",
      " [[11.  0.  0.]\n",
      " [ 0.  7.  4.]\n",
      " [ 0.  4.  4.]] \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  9 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  0.8333333333333334 \n",
      "\n",
      " Confusion Martix: \n",
      " [[12.  0.  0.]\n",
      " [ 0.  9.  2.]\n",
      " [ 0.  3.  4.]] \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  2 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  0.8 \n",
      "\n",
      " Confusion Martix: \n",
      " [[ 5.  0.  0.]\n",
      " [ 0. 11.  3.]\n",
      " [ 0.  3.  8.]] \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  5 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  1.0 \n",
      "\n",
      " Confusion Martix: \n",
      " [[11.  0.  0.]\n",
      " [ 0.  7.  0.]\n",
      " [ 0.  0. 12.]] \n",
      "\n",
      "=== Final Cross-val on test on this fold with === \n",
      " Best NN:  8 \n",
      " Best Dist:  euclidean \n",
      " Best Accuracy:  0.8666666666666667 \n",
      "\n",
      " Confusion Martix: \n",
      " [[11.  0.  0.]\n",
      " [ 0.  7.  0.]\n",
      " [ 0.  4.  8.]] \n",
      "\n",
      " Total Confusion Matrix:  \n",
      " [[50.  0.  0.]\n",
      " [ 0. 41.  9.]\n",
      " [ 0. 14. 36.]] \n",
      "\n",
      " Accuracy List:  \n",
      " [0.7333333333333333, 0.8333333333333334, 0.8, 1.0, 0.8666666666666667] \n",
      "\n",
      " Mean:  0.8466666666666667 \n",
      "\n",
      " Stadard Deviation:  0.08844332774281068\n",
      "____________________________________________________________________________________________________\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# nested cross validation function\n",
    "# X : data / features\n",
    "# y : outputs\n",
    "# foldK - number of folds\n",
    "# nns - list of number of neighbours parameter for validation\n",
    "# dists - list of distances for validation\n",
    "# mySeed - random seed\n",
    "# returns: accuracy over 5 folds (list)\n",
    "\n",
    "def nestedCrossVal(X,y,foldK,nns,dists, mySeed):\n",
    "    np.random.seed(mySeed)\n",
    "    accuracy_fold=[]\n",
    "    totalConfMat=np.zeros((3,3))\n",
    "    \n",
    "    # Randomizes indicies then splits the data into k folds\n",
    "    indices = np.random.permutation(list(range(X.shape[0])))\n",
    "    bins = np.split(indices, foldK)\n",
    "    #print(bins)\n",
    "    \n",
    "    def distToNum(dist):\n",
    "        num = 0;\n",
    "        if(dist == 'euclidean'):\n",
    "            num = 0\n",
    "        if(dist == 'manhattan'):\n",
    "            num = 1\n",
    "        if(dist == 'chebyshev'):\n",
    "            num = 2\n",
    "        if(dist == 'minkowski'):\n",
    "            num = 3\n",
    "        return num\n",
    "    \n",
    "    # loop through folds\n",
    "    for i in range(0,foldK):\n",
    "        foldTrain=[] # list to save current indices for training\n",
    "        foldTest=[]  # list to save current indices for testing\n",
    "        foldVal=[]   # list to save current indices for validation\n",
    "\n",
    "        # loops through all bins, takes bin i for testing, bin i+1 for validation and the rest for testing\n",
    "        for j in range(0,foldK):\n",
    "            if(i==j):\n",
    "                foldTest=bins[i]\n",
    "            elif(i == foldK-1 and j == 0): # wrap around for validation\n",
    "                foldVal=bins[0]\n",
    "            elif(i+1 == j):                   \n",
    "                foldVal=bins[j]\n",
    "            else:\n",
    "                foldTrain.extend(bins[j])\n",
    "            \n",
    "        #print('** Train', len(foldTrain), foldTrain)\n",
    "        #print('** Val', len(foldVal), foldVal)\n",
    "        #print('** Test', len(foldTest), foldTest)\n",
    "        \n",
    "        bestDistance=''  # stores the best distance metric here\n",
    "        bestNN=-10       # stores the best number of neighbours here\n",
    "        bestAccuracy=-10 # stores the best attained accuracy here (in terms of validation)\n",
    "        \n",
    "        # loops through all parameters (one for loop for distances, one for loop for nn)\n",
    "        # trains the classifier on current number of neighbours/distance\n",
    "        # obtains accuracy on validation set\n",
    "        # stores parameters if result are the most accurate\n",
    "        optionList = []\n",
    "        for d in range(0,len(dists)):            \n",
    "            for n in range(0, len(nns)):\n",
    "                optionList = [nns[n], distToNum(dists[d])]\n",
    "                y_pred = mykNN(X[foldTrain], y[foldTrain], X[foldVal], optionList)\n",
    "                acc = myAccuracy(y[foldVal], y_pred)\n",
    "                if(acc > bestAccuracy):\n",
    "                    bestDistance = dists[d]\n",
    "                    bestNN = nns[n]\n",
    "                    bestAccuracy = acc\n",
    "        \n",
    "        #print('** End of val for this fold, best NN', bestNN, 'best Dist', bestDistance)\n",
    "        \n",
    "        #extend your training set by including the validation set\n",
    "        foldTrain.extend(foldVal)\n",
    "        \n",
    "        optionList = [bestNN,distToNum(bestDistance)]\n",
    "        y_pred = mykNN(X[foldTrain], y[foldTrain], X[foldTest], optionList)        \n",
    "        accuracy_fold.append(myAccuracy(y[foldTest], y_pred))\n",
    "        confMat = myConfMat(y[foldTest], y_pred, len(np.unique(y)))\n",
    "        totalConfMat += confMat\n",
    "        \n",
    "        print('=== Final Cross-val on test on this fold with ===','\\n',\n",
    "              'Best NN: ',bestNN,'\\n',\n",
    "              'Best Dist: ',bestDistance,'\\n',\n",
    "              'Best Accuracy: ',myAccuracy(y[foldTest],y_pred),'\\n')\n",
    "        print(' Confusion Martix:','\\n',confMat, '\\n')\n",
    "        if(i == foldK-1):\n",
    "            print(' Total Confusion Matrix: ','\\n',totalConfMat,'\\n')\n",
    "            print(' Accuracy List: ','\\n',accuracy_fold,'\\n')\n",
    "            print(' Mean: ',np.mean(accuracy_fold),'\\n')\n",
    "            print(' Stadard Deviation: ',np.std(accuracy_fold))\n",
    "            for i in range(0,100):\n",
    "                sys.stdout.write('_')\n",
    "            print('\\n\\n')\n",
    "    return accuracy_fold;\n",
    "    \n",
    "# Nested crossvalidation on original data:\n",
    "print('=== Original Data ===','\\n')\n",
    "accuracy_fold=nestedCrossVal(X,y,5,list(range(1,11)),['euclidean','manhattan'], mySeed)\n",
    "\n",
    "\n",
    "# Nested crossvalidation on noisy data:\n",
    "print('=== Noisy Data ===','\\n')\n",
    "accuracy_fold=nestedCrossVal(XN,y,5,list(range(1,11)),['euclidean','manhattan'], mySeed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "nteract": {
   "version": "0.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
